{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 导入相应的库\n",
    "import pandas as pd\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from scipy.sparse import hstack\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "import datetime\n",
    "import numpy as np\n",
    "import io\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from keras.models import Model\n",
    "from keras.layers import Input, Embedding, Dense, Conv2D, MaxPool2D,LSTM,Bidirectional,Activation,Conv1D,GRU\n",
    "from keras.layers import Reshape, Flatten, Concatenate, concatenate,Dropout, SpatialDropout1D\n",
    "from keras.layers import GlobalMaxPooling1D, MaxPooling1D, Add, Flatten,GlobalAveragePooling1D\n",
    "from keras.preprocessing import text, sequence\n",
    "from keras.callbacks import Callback\n",
    "import os\n",
    "from keras import initializers, regularizers, constraints, optimizers, layers, callbacks\n",
    "from keras.callbacks import EarlyStopping,ModelCheckpoint\n",
    "from keras.optimizers import Adam\n",
    "# 计时开始\n",
    "starttime = datetime.datetime.now()\n",
    "\n",
    "# 加载训练集和测试集数据\n",
    "train = pd.read_csv('train.csv').fillna(' ')\n",
    "test = pd.read_csv('test.csv').fillna(' ')\n",
    "class_names = ['toxic', 'severe_toxic', 'obscene', 'threat', 'insult', 'identity_hate']\n",
    "text_name = 'comment_text'\n",
    "\n",
    "def LR():\n",
    "    ''' tfidf+LR '''\n",
    "    print(\"start LR...\")\n",
    "    # 加载训练集文本数据，测试集文本数据\n",
    "    X_train = train[text_name]\n",
    "    X_test = test[text_name]\n",
    "    # 合并训练集和测试集文本，用于训练TF-IDF模型\n",
    "    X_all = pd.concat([X_train, X_test])\n",
    "    # 创建TF-IDF模型，用于计算单个单词的tfidf\n",
    "    word_vectorizer = TfidfVectorizer(\n",
    "        sublinear_tf=True,\n",
    "        strip_accents='unicode',\n",
    "        analyzer='word',\n",
    "        ngram_range=(1, 1),\n",
    "        token_pattern=r'\\w{1,}',\n",
    "        stop_words='english',\n",
    "        max_features=10000)\n",
    "    word_vectorizer.fit(X_all)\n",
    "    # 提取文本单词的特征\n",
    "    x_word_train = word_vectorizer.transform(X_train)\n",
    "    x_word_test = word_vectorizer.transform(X_test)\n",
    "\n",
    "    # 创建TF-IDF模型，用于计算字母和字母组合的tfidf\n",
    "    char_vectorizer = TfidfVectorizer(\n",
    "        sublinear_tf=True,\n",
    "        strip_accents='unicode',\n",
    "        analyzer='char',\n",
    "        stop_words='english',\n",
    "        ngram_range=(2, 6),\n",
    "        max_features=50000)\n",
    "    char_vectorizer.fit(X_all)\n",
    "    # 提取字母和字母组合的tfidf\n",
    "    x_char_train = char_vectorizer.transform(X_train)\n",
    "    x_char_test = char_vectorizer.transform(X_test)\n",
    "    # 将单词的tfidf和字母及其组合的tfidf合并，作为文本的最终特征\n",
    "    x_train = hstack([x_char_train, x_word_train])\n",
    "    x_test = hstack([x_char_test, x_word_test])\n",
    "\n",
    "    scores = []\n",
    "    submission = pd.DataFrame.from_dict({'id': test['id']})\n",
    "    for class_name in class_names:\n",
    "        # y值\n",
    "        y_train = train[class_name]\n",
    "        # 模型\n",
    "        classifier = LogisticRegression(solver='sag')\n",
    "        # 交叉验证\n",
    "        cv_score = np.mean(cross_val_score(classifier, x_train, y_train, cv=3, scoring='roc_auc'))\n",
    "        scores.append(cv_score)\n",
    "        print('LR score for class {} is {}'.format(class_name, cv_score))\n",
    "        # 训练\n",
    "        classifier.fit(x_train, y_train)\n",
    "        # 预测\n",
    "        submission[class_name] = classifier.predict_proba(x_test)[:, 1]\n",
    "        \n",
    "    print('Total LR score is {}'.format(np.mean(scores)))\n",
    "    print('end LR...')\n",
    "    return submission;\n",
    "\n",
    "    # 保存结果到csv文件中\n",
    "max_features = 100000 # 最大特征数，现有数据中所有不同单词的种数\n",
    "maxlen = 200 # 一条评论的词种类数的最大限制\n",
    "embed_size = 300 # 预训练词向量的维度\n",
    "\n",
    "def open_file(fname):\n",
    "    embeddings_index = {}\n",
    "    with open(fname,encoding='utf8') as f:\n",
    "        for line in f:\n",
    "            values = line.rstrip().rsplit(' ')\n",
    "            word = values[0]\n",
    "            coefs = np.asarray(values[1:], dtype='float32')\n",
    "            embeddings_index[word] = coefs\n",
    "    return embeddings_index\n",
    "\n",
    "def getEmbeddingMatrix(fname):\n",
    "    X_train = train[text_name].values\n",
    "    X_test = test[text_name].values\n",
    "    embeddings_index = open_file(fname)\n",
    "    ## 将评论数据，转转成sequences形式，评论中英文单词类别数最大为200 \n",
    "    # 分词器\n",
    "    tokenizer = text.Tokenizer(num_words=max_features)\n",
    "    tokenizer.fit_on_texts(list(X_train) + list(X_test))\n",
    "    # 将评论数据转换成sequences，[1,2,3]\n",
    "    X_train = tokenizer.texts_to_sequences(X_train)\n",
    "    X_test = tokenizer.texts_to_sequences(X_test)\n",
    "    # 统一长度\n",
    "    X_train = sequence.pad_sequences(X_train, maxlen=maxlen)\n",
    "    X_test = sequence.pad_sequences(X_test, maxlen=maxlen)\n",
    "    \n",
    "    word_index = tokenizer.word_index\n",
    "    num_words = min(max_features, len(word_index))\n",
    "    embedding_matrix = np.zeros((num_words, embed_size))\n",
    "    for word, i in word_index.items():\n",
    "        if i >= max_features:continue\n",
    "        embedding_vector = embeddings_index.get(word)\n",
    "        if embedding_vector is not None:embedding_matrix[i] = embedding_vector\n",
    "    return X_train,X_test,embedding_matrix\n",
    "\n",
    "class RocAucEvaluation(Callback):\n",
    "    \"\"\" Callback子类，用于打印ROC-AUC分数 \"\"\"\n",
    "    def __init__(self, validation_data=(), interval=1):\n",
    "        super(Callback, self).__init__()\n",
    "        self.interval = interval\n",
    "        self.X_val, self.y_val = validation_data\n",
    "\n",
    "    def on_epoch_end(self, epoch, logs={}):\n",
    "        if epoch % self.interval == 0:\n",
    "            y_pred = self.model.predict(self.X_val, verbose=0)\n",
    "            score = roc_auc_score(self.y_val, y_pred)\n",
    "            print(\"\\n ROC-AUC - epoch: %d - score: %.6f \\n\" % (epoch+1, score))\n",
    "\n",
    "def textCNN():\n",
    "    ''' crawl-300d-2M + CNN'''\n",
    "    print('star CNN...')\n",
    "    num_filters = 32 # 过滤器数\n",
    "    # 数据\n",
    "    Y_train = train[class_names].values\n",
    "    X_train,X_test,embedding_matrix = getEmbeddingMatrix(\"crawl-300d-2M.vec\")\n",
    "            \n",
    "    inp = Input(shape=(maxlen, ))\n",
    "    # 引入预训练词向量，向量化输入的int，得到max_features * embed_size的矩阵\n",
    "    x = Embedding(max_features, embed_size, weights=[embedding_matrix])(inp)\n",
    "    # 随机丢弃词，提高训练速度，提高词的独立性\n",
    "    x = SpatialDropout1D(0.2)(x)\n",
    "    # 转换维度，添加第三维，维度是1\n",
    "    x = Reshape((maxlen, embed_size, 1))(x)\n",
    "    # 卷积层，过滤器32,大小1*300\n",
    "    conv_1 = Conv2D(num_filters, kernel_size=(1, embed_size), kernel_initializer='normal',\n",
    "                                                                                    activation='elu')(x)\n",
    "    conv_2 = Conv2D(num_filters, kernel_size=(2, embed_size), kernel_initializer='normal',\n",
    "                                                                                    activation='elu')(x)\n",
    "    conv_3 = Conv2D(num_filters, kernel_size=(3, embed_size), kernel_initializer='normal',\n",
    "                                                                                    activation='elu')(x)\n",
    "    conv_5 = Conv2D(num_filters, kernel_size=(5, embed_size), kernel_initializer='normal',\n",
    "                                                                                    activation='elu')(x)\n",
    "    # 最大池化层\n",
    "    maxpool_1 = MaxPool2D(pool_size=(maxlen, 1))(conv_1)\n",
    "    maxpool_2 = MaxPool2D(pool_size=(maxlen - 1, 1))(conv_2)\n",
    "    maxpool_3 = MaxPool2D(pool_size=(maxlen - 2, 1))(conv_3)\n",
    "    maxpool_5 = MaxPool2D(pool_size=(maxlen - 4, 1))(conv_5)\n",
    "    # 连接最大池化层\n",
    "    z = Concatenate(axis=1)([maxpool_1, maxpool_2,maxpool_3,maxpool_5])   \n",
    "    # 压平\n",
    "    z = Flatten()(z)\n",
    "    # 随机丢弃，提高最后训练速度，防止因全连接层导致过拟合\n",
    "    z = Dropout(0.1)(z)\n",
    "    # 全连接层，输入六个值\n",
    "    outp = Dense(6, activation=\"sigmoid\")(z)\n",
    "    \n",
    "    model = Model(inputs=inp, outputs=outp)\n",
    "    model.compile(loss='binary_crossentropy',\n",
    "                  optimizer='adam',\n",
    "                  metrics=['accuracy'])\n",
    "    # 拆分训练集和验证集\n",
    "    x_train, x_val, y_train, y_val = train_test_split(X_train, Y_train, train_size=0.96)\n",
    "    # 模型评估\n",
    "    RocAuc = RocAucEvaluation(validation_data=(x_val, y_val), interval=1)\n",
    "    # 训练\n",
    "    # verbose：日志显示，0为不在标准输出流输出日志信息，1为输出进度条记录，2为每个epoch输出一行记录\n",
    "    hist = model.fit(x_train, y_train, batch_size=256, epochs=3, validation_data=(x_val, y_val),\n",
    "                 callbacks=[RocAuc], verbose=2)\n",
    "    # 预测\n",
    "    y_pred = model.predict(X_test)\n",
    "    submission = pd.read_csv('sample_submission.csv')\n",
    "    submission[class_names] = y_pred\n",
    "    print('end CNN...')\n",
    "    return submission\n",
    "\n",
    "def textRNN():\n",
    "    ''' glove.840B.300d + RNN'''\n",
    "    print('start RNN...')\n",
    "    # 数据\n",
    "    Y_train = train[class_names].values\n",
    "    X_train,X_test,embedding_matrix = getEmbeddingMatrix(\"glove.840B.300d.txt\")\n",
    "    \n",
    "    sequence_input = Input(shape=(maxlen, ))\n",
    "    x = Embedding(max_features, embed_size, weights=[embedding_matrix],trainable = False)(sequence_input)\n",
    "    x = SpatialDropout1D(0.2)(x)\n",
    "    x = Bidirectional(GRU(128, return_sequences=True,dropout=0.1,recurrent_dropout=0.1))(x)\n",
    "    x = Conv1D(64, kernel_size = 3, padding = \"valid\", kernel_initializer = \"glorot_uniform\")(x)\n",
    "    avg_pool = GlobalAveragePooling1D()(x)\n",
    "    max_pool = GlobalMaxPooling1D()(x)\n",
    "    x = concatenate([avg_pool, max_pool]) \n",
    "    preds = Dense(6, activation=\"sigmoid\")(x)\n",
    "    model = Model(sequence_input, preds)\n",
    "    model.compile(loss='binary_crossentropy',optimizer=Adam(lr=1e-3),metrics=['accuracy'])\n",
    "\n",
    "    x_train, x_val, y_train, y_val = train_test_split(X_train, Y_train, train_size=0.96)\n",
    "    RocAuc = RocAucEvaluation(validation_data=(x_val, y_val), interval=1)\n",
    "\n",
    "    model.fit(x_train, y_train, batch_size=128, epochs=4, validation_data=(x_val, y_val),callbacks = [RocAuc],verbose=2)\n",
    "    y_pred = model.predict(X_test,batch_size=1024,verbose=1)\n",
    "\n",
    "    submission = pd.read_csv('sample_submission.csv')\n",
    "    submission[class_names] = y_pred\n",
    "    print('end RNN...')\n",
    "    return submission\n",
    "\n",
    "# 执行\n",
    "lr = LR()\n",
    "lr.to_csv('submission_LR.csv', index = False)\n",
    "\n",
    "cnn = textCNN()\n",
    "cnn.to_csv('submission_CNN.csv', index = False)\n",
    "\n",
    "rnn = textRNN()\n",
    "rnn.to_csv('submission_RNN.csv', index = False)\n",
    "\n",
    "blend = lr.copy()\n",
    "col = lr.columns\n",
    "col = col.tolist()\n",
    "col.remove('id')\n",
    "# 加权平均\n",
    "for i in col:\n",
    "    blend[i] = (3* lr[i] + 2 * cnn[i] + 5 * rnn[i]) / 10\n",
    "    \n",
    "blend.to_csv('325submission.csv', index = False)\n",
    "endtime = datetime.datetime.now()\n",
    "print (endtime - starttime)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
